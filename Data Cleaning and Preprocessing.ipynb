{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "FHVvLEBo2bzo",
      "metadata": {
        "id": "FHVvLEBo2bzo"
      },
      "source": [
        "# Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2fjMhyeDrHeq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fjMhyeDrHeq",
        "outputId": "b008f2ab-f109-487e-ce0d-374d8c1020d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "qfKPtkgKqa0s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfKPtkgKqa0s",
        "outputId": "6072f2fc-08e6-4be2-f036-8c96e42e50e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E-WeZvyF2ftH",
      "metadata": {
        "id": "E-WeZvyF2ftH"
      },
      "source": [
        "We are going to be using the datasets for continuous bag of words (cBoW) and skipagrams from the AraVec2 corpus, so we will download and unzip them for usage later on in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "xAII9aufse4g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAII9aufse4g",
        "outputId": "d77f3725-b727-4784-fdfd-4da98b32510f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-21 13:04:46--  https://archive.org/download/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia803107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip [following]\n",
            "--2025-04-21 13:04:48--  https://ia803107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving ia803107.us.archive.org (ia803107.us.archive.org)... 207.241.232.157\n",
            "Connecting to ia803107.us.archive.org (ia803107.us.archive.org)|207.241.232.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 364888893 (348M) [application/zip]\n",
            "Saving to: ‘wiki_cbow_300.zip’\n",
            "\n",
            "wiki_cbow_300.zip   100%[===================>] 347.98M  7.61MB/s    in 23s     \n",
            "\n",
            "2025-04-21 13:05:12 (15.0 MB/s) - ‘wiki_cbow_300.zip’ saved [364888893/364888893]\n",
            "\n",
            "Archive:  ./wiki_cbow_300.zip\n",
            "  inflating: ./word2vec_model/wikipedia_cbow_300  \n",
            "  inflating: ./word2vec_model/wikipedia_cbow_300.trainables.syn1neg.npy  \n",
            "  inflating: ./word2vec_model/wikipedia_cbow_300.wv.vectors.npy  \n",
            "--2025-04-21 13:05:16--  https://archive.org/download/aravec2.0/wiki_sg_300.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia903107.us.archive.org/0/items/aravec2.0/wiki_sg_300.zip [following]\n",
            "--2025-04-21 13:05:17--  https://ia903107.us.archive.org/0/items/aravec2.0/wiki_sg_300.zip\n",
            "Resolving ia903107.us.archive.org (ia903107.us.archive.org)... 207.241.232.147\n",
            "Connecting to ia903107.us.archive.org (ia903107.us.archive.org)|207.241.232.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 364822526 (348M) [application/zip]\n",
            "Saving to: ‘wiki_sg_300.zip’\n",
            "\n",
            "wiki_sg_300.zip     100%[===================>] 347.92M  8.70MB/s    in 24s     \n",
            "\n",
            "2025-04-21 13:05:41 (14.5 MB/s) - ‘wiki_sg_300.zip’ saved [364822526/364822526]\n",
            "\n",
            "Archive:  ./wiki_sg_300.zip\n",
            "  inflating: ./word2vec_model/wikipedia_sg_300  \n",
            "  inflating: ./word2vec_model/wikipedia_sg_300.trainables.syn1neg.npy  \n",
            "  inflating: ./word2vec_model/wikipedia_sg_300.wv.vectors.npy  \n"
          ]
        }
      ],
      "source": [
        "!wget \"https://archive.org/download/aravec2.0/wiki_cbow_300.zip\"\n",
        "!unzip \"./wiki_cbow_300.zip\" -d \"./word2vec_model\"\n",
        "!wget \"https://archive.org/download/aravec2.0/wiki_sg_300.zip\"\n",
        "!unzip \"./wiki_sg_300.zip\" -d \"./word2vec_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "uD9ooZi8dOnc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uD9ooZi8dOnc",
        "outputId": "eb7813b7-64a6-4f77-c7b7-f816d10d98a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting numpy>=1.23.2 (from pandas)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "72d7abf2010f4ba69045c9854bd8cb74",
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "pytz",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n",
        "!pip install --upgrade --force-reinstall pandas\n",
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9d2f9c62",
      "metadata": {
        "id": "9d2f9c62"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, GRU, Input, Dense, Embedding, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adfb71c",
      "metadata": {
        "id": "7adfb71c"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N12xCnVdfclS",
      "metadata": {
        "id": "N12xCnVdfclS"
      },
      "source": [
        "The first step is to clean the data we have in the data.txt file, and remove any noise from the data for easier training and modeling. This code is split into two classes, the TextCleaner and the DataReader.\n",
        "\n",
        "The TextCleaner cleans the Arabic text by normalizing the text, removing symbols, removing tashkeel, remove unnecessary symbols, fixes longations, removes empty and whitespaces.\n",
        "\n",
        "The DataReader class Reads and processes labeled text data for sequence labeling, specifically NER, tasks. It outputs a list of sentences where each sentence is a list of words, tags and a set of unique cleaned words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "235fa431",
      "metadata": {
        "id": "235fa431"
      },
      "outputs": [],
      "source": [
        "class TextCleaner:\n",
        "\n",
        "    def remove_line(self, entity):\n",
        "        entity = re.sub('\\n','',entity)\n",
        "        return entity\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        # List of characters to search and their replacements to normalize text\n",
        "        search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "        replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "\n",
        "        # Remove tashkeel\n",
        "        p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "        text = re.sub(p_tashkeel,\"\", text)\n",
        "\n",
        "        # Remove longation\n",
        "        p_longation = re.compile(r'(.)\\1+')\n",
        "        subst = r\"\\1\\1\"\n",
        "        text = re.sub(p_longation, subst, text)\n",
        "\n",
        "        for i in range(0, len(search)):\n",
        "            text = text.replace(search[i], replace[i])\n",
        "\n",
        "        # Trim\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def re_clean(self, old_sentence, old_tags):\n",
        "        space_regex = re.compile(\"\\s+\")\n",
        "        new_sentence = []\n",
        "        new_tags = []\n",
        "        for j in range(len(old_sentence)):\n",
        "            # add word if not empty and doesn't contain spaces only\n",
        "            if old_sentence[j]!=\"\" and space_regex.match(old_sentence[j])==None:\n",
        "                new_sentence.append(old_sentence[j])\n",
        "                new_tags.append(old_tags[j])\n",
        "        return new_sentence, new_tags\n",
        "\n",
        "\n",
        "class DataReader:\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    def read_data_rnn(self):\n",
        "        # Reads the data file, cleans the sentences, and separates words and tags.\n",
        "        cleaner = TextCleaner()\n",
        "\n",
        "        sentences = []\n",
        "        tags = []\n",
        "        vocab = set()\n",
        "\n",
        "        data_txt = open(self.path,'r',encoding='utf-8')\n",
        "        sentence = []\n",
        "        entity = []\n",
        "        # Each line is split into word and entity\n",
        "        for line in data_txt.readlines():\n",
        "            if line == '\\n':\n",
        "                recleaned = cleaner.re_clean(sentence, entity)\n",
        "                sentences.append(recleaned[0].copy())\n",
        "                tags.append(recleaned[1].copy())\n",
        "                sentence.clear()\n",
        "                entity.clear()\n",
        "            else:\n",
        "                line = line.split(sep=' ')\n",
        "                clean_word = cleaner.clean_text(line[0])\n",
        "                vocab.add(clean_word)\n",
        "                sentence.append(clean_word)\n",
        "                entity.append(cleaner.remove_line(line[1]))\n",
        "        print('Sentences:', len(sentences), ', Tags:', len(tags), ', Unique Words:', len(vocab))\n",
        "        return sentences, tags, vocab"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
